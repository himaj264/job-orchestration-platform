version: '3.8'

# ============================================================================
# Event-Driven Distributed Job Orchestration Platform
# Docker Compose Configuration - Optimized for 8GB RAM
# ============================================================================

services:
  # --------------------------------------------------------------------------
  # ZOOKEEPER - Kafka Coordination Service
  # --------------------------------------------------------------------------
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    hostname: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      # Memory optimization for 8GB RAM systems
      KAFKA_HEAP_OPTS: "-Xmx256m -Xms256m"
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "2181"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - job-platform-network
    restart: unless-stopped

  # --------------------------------------------------------------------------
  # KAFKA - Message Broker
  # --------------------------------------------------------------------------
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    hostname: kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      # Listener configuration for both internal and external access
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      # Memory optimization for 8GB RAM systems
      KAFKA_HEAP_OPTS: "-Xmx512m -Xms512m"
      # Performance tuning
      KAFKA_NUM_PARTITIONS: 3
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS: 300000
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 10s
      timeout: 10s
      retries: 10
      start_period: 30s
    networks:
      - job-platform-network
    restart: unless-stopped

  # --------------------------------------------------------------------------
  # KAFKA TOPIC INITIALIZATION
  # --------------------------------------------------------------------------
  kafka-init:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka-init
    depends_on:
      kafka:
        condition: service_healthy
    entrypoint: ["/bin/sh", "-c"]
    command: |
      "
      echo 'Waiting for Kafka to be ready...'
      sleep 5

      echo 'Creating Kafka topics...'

      # Create job-requests topic (main job queue)
      kafka-topics --create --if-not-exists \
        --bootstrap-server kafka:29092 \
        --topic job-requests \
        --partitions 3 \
        --replication-factor 1 \
        --config retention.ms=604800000

      # Create job-status topic (status updates)
      kafka-topics --create --if-not-exists \
        --bootstrap-server kafka:29092 \
        --topic job-status \
        --partitions 3 \
        --replication-factor 1 \
        --config retention.ms=604800000

      # Create job-dlq topic (dead letter queue)
      kafka-topics --create --if-not-exists \
        --bootstrap-server kafka:29092 \
        --topic job-dlq \
        --partitions 1 \
        --replication-factor 1 \
        --config retention.ms=2592000000

      echo 'Topics created successfully!'
      kafka-topics --list --bootstrap-server kafka:29092
      "
    networks:
      - job-platform-network

  # --------------------------------------------------------------------------
  # REDIS - Caching & Idempotency
  # --------------------------------------------------------------------------
  redis:
    image: redis:7.2-alpine
    container_name: redis
    hostname: redis
    ports:
      - "6379:6379"
    command: >
      redis-server
      --maxmemory 128mb
      --maxmemory-policy allkeys-lru
      --appendonly yes
      --appendfsync everysec
    volumes:
      - redis-data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - job-platform-network
    restart: unless-stopped

  # --------------------------------------------------------------------------
  # POSTGRESQL - Job Metadata Storage
  # --------------------------------------------------------------------------
  postgres:
    image: postgres:15-alpine
    container_name: postgres
    hostname: postgres
    ports:
      - "5432:5432"
    environment:
      POSTGRES_DB: jobdb
      POSTGRES_USER: jobuser
      POSTGRES_PASSWORD: jobpassword
      # Memory optimization
      POSTGRES_SHARED_BUFFERS: 128MB
      POSTGRES_EFFECTIVE_CACHE_SIZE: 256MB
      POSTGRES_WORK_MEM: 4MB
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./init-db.sql:/docker-entrypoint-initdb.d/init-db.sql:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U jobuser -d jobdb"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - job-platform-network
    restart: unless-stopped

  # --------------------------------------------------------------------------
  # ORCHESTRATOR SERVICE - Job Management API
  # --------------------------------------------------------------------------
  orchestrator:
    build:
      context: ./orchestrator-service
      dockerfile: Dockerfile
    container_name: orchestrator
    hostname: orchestrator
    depends_on:
      postgres:
        condition: service_healthy
      kafka:
        condition: service_healthy
      kafka-init:
        condition: service_completed_successfully
    ports:
      - "8080:8080"
    environment:
      SPRING_PROFILES_ACTIVE: docker
      SPRING_DATASOURCE_URL: jdbc:postgresql://postgres:5432/jobdb
      SPRING_DATASOURCE_USERNAME: jobuser
      SPRING_DATASOURCE_PASSWORD: jobpassword
      SPRING_KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      # JVM Memory optimization for 8GB RAM systems
      JAVA_OPTS: "-Xmx384m -Xms256m -XX:+UseG1GC -XX:MaxGCPauseMillis=100"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/actuator/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - job-platform-network
    restart: unless-stopped

  # --------------------------------------------------------------------------
  # WORKER SERVICE - Job Execution Engine
  # --------------------------------------------------------------------------
  worker:
    build:
      context: ./worker-service
      dockerfile: Dockerfile
    hostname: worker
    depends_on:
      kafka:
        condition: service_healthy
      kafka-init:
        condition: service_completed_successfully
      redis:
        condition: service_healthy
    ports:
      - "8081-8083:8081"
    environment:
      SPRING_PROFILES_ACTIVE: docker
      SPRING_KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      SPRING_DATA_REDIS_HOST: redis
      SPRING_DATA_REDIS_PORT: 6379
      # Worker configuration
      WORKER_CONCURRENCY: 3
      WORKER_RETRY_MAX_ATTEMPTS: 3
      WORKER_RETRY_BACKOFF_INITIAL: 1000
      WORKER_RETRY_BACKOFF_MULTIPLIER: 2
      # JVM Memory optimization
      JAVA_OPTS: "-Xmx256m -Xms128m -XX:+UseG1GC -XX:MaxGCPauseMillis=100"
    networks:
      - job-platform-network
    restart: unless-stopped
    deploy:
      mode: replicated
      replicas: 1

# ============================================================================
# NETWORKS
# ============================================================================
networks:
  job-platform-network:
    driver: bridge
    name: job-platform-network

# ============================================================================
# VOLUMES
# ============================================================================
volumes:
  postgres-data:
    name: job-platform-postgres-data
  redis-data:
    name: job-platform-redis-data
